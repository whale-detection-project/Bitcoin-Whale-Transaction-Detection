import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.cluster import KMeans
import xgboost as xgb
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# ğŸ”¹ 1. í•™ìŠµ ë°ì´í„° ë¡œë“œ
train_df = pd.read_csv("dataset/1000btc_train.csv")
features = ['input_count', 'output_count', 'max_output_ratio', 'fee_per_max_ratio', 'max_input_ratio']

# ğŸ”¹ 2. ë¡œê·¸ ë³€í™˜ + ì •ê·œí™”
X_train_log = train_df[features].apply(lambda x: np.log1p(x))
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_log)

# ğŸ”¹ 3. í´ëŸ¬ìŠ¤í„° ë¼ë²¨ ìƒì„±
kmeans = KMeans(n_clusters=4, random_state=42)
train_df['cluster_label'] = kmeans.fit_predict(X_train_scaled)

# ğŸ”¹ 4. PCA í•™ìŠµ (ì‹œê°í™” ë° ì‹¤ì‹œê°„ íƒì§€ ìš©ë„)
pca = PCA(n_components=2)
pca.fit(X_train_scaled)

# ğŸ”¹ 5. í•™ìŠµ ì¤€ë¹„
X = X_train_scaled
y = train_df['cluster_label']

# ğŸ”¹ 6. í´ë˜ìŠ¤ë³„ ìˆ˜ë™ ê°€ì¤‘ì¹˜
manual_class_weights = {0: 4.0, 1: 0.5, 2: 4.0, 3: 3.0}
sample_weights = np.array([manual_class_weights[label] for label in y])

# ğŸ”¹ 7. ëª¨ë¸ í•™ìŠµ
xgb_model = xgb.XGBClassifier(
    n_estimators=200,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42,
    eval_metric='mlogloss',
    tree_method='hist'
)
xgb_model.fit(X, y, sample_weight=sample_weights)

# ğŸ”¹ 8. ëª¨ë¸ ë° ì „ì²˜ë¦¬ê¸° ì €ì¥
joblib.dump(xgb_model, "model/xgb_model.pkl")
xgb_model.save_model("model/xgb_model.json")
joblib.dump(scaler, "model/scaler.pkl")
joblib.dump(kmeans, "model/kmeans.pkl")
joblib.dump(pca, "model/pca.pkl")

print("âœ… ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, í´ëŸ¬ìŠ¤í„°ë§, PCA ì €ì¥ ì™„ë£Œ")
