import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from xgboost import XGBClassifier
import numpy as np
import joblib

# CSV ë¡œë“œ
df = pd.read_csv("labeled_whales.csv")

# ğŸ‹ ê³ ë˜ ë¼ë²¨ ì •ì˜ (0 í¬í•¨)
whale_labels = {
    0: 'normal',
    1: 'one_input_whale',
    2: 'distributed_whale',
    3: 'dust_merging_whale',
    4: 'fast_transfer_whale',
    5: 'clean_hide_whale'
}

# ë¼ë²¨ ì¸ì½”ë”© (0~5 â†’ 0~5 ê·¸ëŒ€ë¡œ)
label_map = {k: k for k in whale_labels.keys()}
reverse_map = {v: k for k, v in label_map.items()}
df['whale_type_encoded'] = df['whale_type'].map(label_map)

# í”¼ì²˜ ë° ë¼ë²¨ ì •ì˜
features = [
    'input_count', 'output_count', 'total_input_value',
    'max_input_value', 'max_output_value', 'max_output_ratio',
    'fee_per_max_ratio', 'has_zero_output'
]
X = df[features].copy()
X['has_zero_output'] = X['has_zero_output'].astype(int)
y = df['whale_type_encoded']

# ë°ì´í„° ë¶„í• 
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# XGBoost ëª¨ë¸ í•™ìŠµ
model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
model.fit(X_train, y_train)

# ì˜ˆì¸¡ ë° í‰ê°€
y_pred = model.predict(X_test)

# ì „ì²´ ë ˆì´ë¸” ë° ì´ë¦„
labels = sorted(whale_labels.keys())  # [0, 1, 2, 3, 4, 5]
target_names = [whale_labels[i] for i in labels]

print("\n[ğŸ“Š Classification Report]")
print(classification_report(y_test, y_pred, labels=labels, target_names=target_names))

# âœ… ëª¨ë¸ ì €ì¥
joblib.dump(model, "dog.joblib")
print("\nâœ… ëª¨ë¸ì´ 'xgb_whale_classifier_with_normal.joblib' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
