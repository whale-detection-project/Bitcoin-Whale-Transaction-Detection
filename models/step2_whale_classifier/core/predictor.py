"""
ğŸ‹ ê³ ë˜ ê±°ë˜ ì˜ˆì¸¡ ì—”ì§„
Random Forest ê¸°ë°˜ ì‹¤ì‹œê°„ ê³ ë˜ íŒ¨í„´ ë¶„ë¥˜
"""

import numpy as np
import pandas as pd
import pickle
import joblib
from pathlib import Path
from typing import Dict, List, Tuple, Optional
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import cross_val_score
import logging
from datetime import datetime

from ..config.settings import (
    MODEL_CONFIG, WHALE_CLASSES, FEATURES, 
    ANALYSIS_CONFIG, DATA_PATH, MODEL_PATH
)

class WhalePredictor:
    """ğŸ‹ ê³ ë˜ ê±°ë˜ íŒ¨í„´ ì˜ˆì¸¡ê¸°"""
    
    def __init__(self):
        self.model: Optional[RandomForestClassifier] = None
        self.scaler: Optional[StandardScaler] = None
        self.feature_names = FEATURES['input_features']
        self.is_trained = False
        self.feature_importance = None
        self.model_metrics = {}
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
        
        # ëª¨ë¸ ì €ì¥ ê²½ë¡œ ìƒì„±
        MODEL_PATH.mkdir(parents=True, exist_ok=True)
    
    def load_training_data(self) -> Tuple[pd.DataFrame, pd.Series]:
        """Step 1ì—ì„œ ìƒì„±ëœ ìµœì í™” ë°ì´í„° ë¡œë“œ"""
        try:
            data_file = DATA_PATH / "optimized_whale_dataset.csv"
            self.logger.info(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì¤‘: {data_file}")
            
            df = pd.read_csv(data_file)
            self.logger.info(f"âœ… ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´")
            
            # í”¼ì²˜ì™€ ë¼ë²¨ ë¶„ë¦¬
            X = df[self.feature_names]
            y = df['whale_class']
            
            # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
            self._validate_data(X, y)
            
            return X, y
            
        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def _validate_data(self, X: pd.DataFrame, y: pd.Series):
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        # ê²°ì¸¡ê°’ í™•ì¸
        if X.isnull().any().any():
            self.logger.warning("âš ï¸ í”¼ì²˜ì— ê²°ì¸¡ê°’ ë°œê²¬")
        
        # í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
        class_dist = y.value_counts().sort_index()
        self.logger.info("ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:")
        for cls, count in class_dist.items():
            whale_name = WHALE_CLASSES[cls]['name']
            percentage = count / len(y) * 100
            self.logger.info(f"   í´ë˜ìŠ¤ {cls} ({whale_name}): {count:,}ê±´ ({percentage:.1f}%)")
    
    def train_model(self, X: pd.DataFrame, y: pd.Series) -> Dict:
        """ëª¨ë¸ í›ˆë ¨ ë° ìµœì í™”"""
        try:
            self.logger.info("ğŸš€ ëª¨ë¸ í›ˆë ¨ ì‹œì‘...")
            
            # ë°ì´í„° ì •ê·œí™”
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X)
            
            # Class Weight ì„¤ì • (Step 1 ìµœì  ì „ëµ ì‚¬ìš©)
            class_weights = {
                cls: WHALE_CLASSES[cls]['weight'] 
                for cls in WHALE_CLASSES.keys()
            }
            
            # Random Forest ëª¨ë¸ ìƒì„±
            model_config = MODEL_CONFIG.copy()
            model_config.pop('class_weight_strategy', None)  # ì œê±°
            model_config['oob_score'] = True  # OOB ì ìˆ˜ ê³„ì‚° í™œì„±í™”
            
            self.model = RandomForestClassifier(
                **model_config,
                class_weight=class_weights
            )
            
            # êµì°¨ ê²€ì¦ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
            self.logger.info("ğŸ“ˆ êµì°¨ ê²€ì¦ ìˆ˜í–‰ ì¤‘...")
            cv_scores = cross_val_score(
                self.model, X_scaled, y, 
                cv=5, scoring='f1_macro', n_jobs=-1
            )
            
            # ëª¨ë¸ í›ˆë ¨
            self.logger.info("ğŸ”§ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            self.model.fit(X_scaled, y)
            
            # í”¼ì²˜ ì¤‘ìš”ë„ ì €ì¥
            self.feature_importance = dict(zip(
                self.feature_names, 
                self.model.feature_importances_
            ))
            
            # ëª¨ë¸ ë©”íŠ¸ë¦­ ì €ì¥
            oob_score = getattr(self.model, 'oob_score_', 0.0)  # ì•ˆì „í•œ ì ‘ê·¼
            self.model_metrics = {
                'cv_f1_mean': cv_scores.mean(),
                'cv_f1_std': cv_scores.std(),
                'oob_score': oob_score,
                'n_estimators': self.model.n_estimators,
                'training_samples': len(X),
                'training_time': datetime.now().isoformat()
            }
            
            self.is_trained = True
            
            self.logger.info("âœ… ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!")
            self.logger.info(f"ğŸ“Š êµì°¨ê²€ì¦ F1-Score: {cv_scores.mean():.4f} (Â±{cv_scores.std():.4f})")
            self.logger.info(f"ğŸ“Š OOB Score: {self.model.oob_score_:.4f}")
            
            return self.model_metrics
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise
    
    def predict_whale_type(self, transaction_data: Dict) -> Dict:
        """ë‹¨ì¼ ê±°ë˜ ë°ì´í„°ë¡œ ê³ ë˜ ìœ í˜• ì˜ˆì¸¡"""
        if not self.is_trained:
            raise ValueError("ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. train_model()ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        
        try:
            # ì…ë ¥ ë°ì´í„° ê²€ì¦ ë° ë³€í™˜
            features = self._prepare_features(transaction_data)
            
            # ì˜ˆì¸¡ ìˆ˜í–‰
            features_scaled = self.scaler.transform([features])
            
            # í´ë˜ìŠ¤ í™•ë¥  ì˜ˆì¸¡
            class_probabilities = self.model.predict_proba(features_scaled)[0]
            predicted_class = self.model.predict(features_scaled)[0]
            confidence = class_probabilities[predicted_class]
            
            # í”¼ì²˜ ê¸°ì—¬ë„ ê³„ì‚° (íŠ¸ë¦¬ ê¸°ë°˜ ì„¤ëª…)
            feature_contributions = self._calculate_feature_contributions(features_scaled[0])
            
            # ê²°ê³¼ êµ¬ì„±
            result = {
                'predicted_class': int(predicted_class),
                'whale_info': WHALE_CLASSES[predicted_class],
                'confidence': float(confidence),
                'class_probabilities': {
                    cls: float(prob) for cls, prob in enumerate(class_probabilities)
                },
                'feature_contributions': feature_contributions,
                'input_features': dict(zip(self.feature_names, features)),
                'prediction_timestamp': datetime.now().isoformat()
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"âŒ ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            raise
    
    def _prepare_features(self, transaction_data: Dict) -> List[float]:
        """ì…ë ¥ ë°ì´í„°ë¥¼ ëª¨ë¸ í”¼ì²˜ë¡œ ë³€í™˜"""
        features = []
        
        for feature_name in self.feature_names:
            if feature_name not in transaction_data:
                raise ValueError(f"í•„ìˆ˜ í”¼ì²˜ ëˆ„ë½: {feature_name}")
            
            value = transaction_data[feature_name]
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            if not isinstance(value, (int, float)):
                raise ValueError(f"í”¼ì²˜ {feature_name}ëŠ” ìˆ«ìì—¬ì•¼ í•©ë‹ˆë‹¤: {value}")
            
            # ë²”ìœ„ ê²€ì¦
            if feature_name == 'concentration' and not (0 <= value <= 1):
                raise ValueError(f"ì§‘ì¤‘ë„ëŠ” 0~1 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤: {value}")
            
            if feature_name in ['input_count', 'output_count'] and value < 1:
                raise ValueError(f"{feature_name}ëŠ” 1 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {value}")
            
            if feature_name in ['total_volume_btc', 'fee_btc'] and value < 0:
                raise ValueError(f"{feature_name}ëŠ” 0 ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤: {value}")
            
            features.append(float(value))
        
        return features
    
    def _calculate_feature_contributions(self, features: np.ndarray) -> Dict:
        """í”¼ì²˜ë³„ ì˜ˆì¸¡ ê¸°ì—¬ë„ ê³„ì‚°"""
        contributions = {}
        
        # í”¼ì²˜ ì¤‘ìš”ë„ì™€ í˜„ì¬ ê°’ì˜ ìƒëŒ€ì  í¬ê¸° ê³ ë ¤
        for i, (feature_name, importance) in enumerate(self.feature_importance.items()):
            # ì •ê·œí™”ëœ í”¼ì²˜ ê°’ê³¼ ì¤‘ìš”ë„ë¥¼ ê²°í•©
            contribution_score = abs(features[i]) * importance
            contributions[feature_name] = {
                'importance': float(importance),
                'normalized_value': float(features[i]),
                'contribution_score': float(contribution_score)
            }
        
        return contributions
    
    def get_similar_transactions(self, transaction_data: Dict, n_similar: int = 5) -> List[Dict]:
        """ìœ ì‚¬í•œ ê±°ë˜ íŒ¨í„´ ê²€ìƒ‰ (ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì‹œë®¬ë ˆì´ì…˜
            
            X, y = self.load_training_data()
            
            # í˜„ì¬ ê±°ë˜ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚° (ìœ í´ë¦¬ë“œ ê±°ë¦¬ ê¸°ë°˜)
            current_features = self._prepare_features(transaction_data)
            X_scaled = self.scaler.transform(X)
            current_scaled = self.scaler.transform([current_features])
            
            # ê±°ë¦¬ ê³„ì‚°
            distances = np.linalg.norm(X_scaled - current_scaled, axis=1)
            similar_indices = np.argsort(distances)[:n_similar]
            
            similar_transactions = []
            for idx in similar_indices:
                similar_data = X.iloc[idx].to_dict()
                similar_class = y.iloc[idx]
                distance = distances[idx]
                
                similar_transactions.append({
                    'features': similar_data,
                    'whale_class': int(similar_class),
                    'whale_name': WHALE_CLASSES[similar_class]['name'],
                    'similarity_score': float(1 / (1 + distance))  # 0~1 ì ìˆ˜
                })
            
            return similar_transactions
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ ìœ ì‚¬ ê±°ë˜ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def save_model(self, filename: str = None):
        """í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥"""
        if not self.is_trained:
            raise ValueError("ì €ì¥í•  í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"whale_classifier_{timestamp}"
        
        try:
            # ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            model_path = MODEL_PATH / f"{filename}_model.pkl"
            scaler_path = MODEL_PATH / f"{filename}_scaler.pkl"
            metrics_path = MODEL_PATH / f"{filename}_metrics.pkl"
            
            joblib.dump(self.model, model_path)
            joblib.dump(self.scaler, scaler_path)
            joblib.dump({
                'metrics': self.model_metrics,
                'feature_importance': self.feature_importance,
                'feature_names': self.feature_names
            }, metrics_path)
            
            self.logger.info(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            raise
    
    def load_model(self, filename: str):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        try:
            model_path = MODEL_PATH / f"{filename}_model.pkl"
            scaler_path = MODEL_PATH / f"{filename}_scaler.pkl"
            metrics_path = MODEL_PATH / f"{filename}_metrics.pkl"
            
            self.model = joblib.load(model_path)
            self.scaler = joblib.load(scaler_path)
            
            if metrics_path.exists():
                metrics_data = joblib.load(metrics_path)
                self.model_metrics = metrics_data['metrics']
                self.feature_importance = metrics_data['feature_importance']
                self.feature_names = metrics_data['feature_names']
            
            self.is_trained = True
            self.logger.info(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            
        except Exception as e:
            self.logger.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def get_model_info(self) -> Dict:
        """ëª¨ë¸ ì •ë³´ ì¡°íšŒ"""
        if not self.is_trained:
            return {"status": "ëª¨ë¸ì´ í›ˆë ¨ë˜ì§€ ì•ŠìŒ"}
        
        return {
            "status": "í›ˆë ¨ ì™„ë£Œ",
            "metrics": self.model_metrics,
            "feature_importance": self.feature_importance,
            "model_config": MODEL_CONFIG,
            "feature_names": self.feature_names
        } 