"""
ğŸŒ³ Step 1: Class Weight ì¡°ì •ì„ í†µí•œ í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°

ëª©ì : 1000 BTC ì´ìƒ ëŒ€í˜• ê±°ë˜ì—ì„œ ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜
- ë°ì´í„°: data/1000btc.csv (888,943ê±´ì˜ ëŒ€í˜• Bitcoin ê±°ë˜)
- ë°©ë²•: ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ìœ¼ë¡œ ë¼ë²¨ë§ í›„ Class Weight ì¡°ì •
- ëª©í‘œ: Random Forest ê¸°ë°˜ ê±°ë˜ í–‰ë™ íŒ¨í„´ ë¶„ë¥˜ ì„±ëŠ¥ ê°œì„ 

ê±°ë˜ í–‰ë™ íŒ¨í„´ ë¶„ë¥˜:
1. ìˆ˜ì§‘í˜• ê³ ë˜: Input > ìƒìœ„ 20% AND Output â‰¤ 2ê°œ (~150,000ê±´, 17%)
2. ë¶„ì‚°í˜• ê³ ë˜: Output > ìƒìœ„ 15% AND ì§‘ì¤‘ë„ < 0.8 (~130,000ê±´, 15%)
3. ê¸‰í–‰í˜• ê³ ë˜: Fee > ìƒìœ„ 5% (44,448ê±´, 5%)
4. ì§‘ì¤‘í˜• ê³ ë˜: ì§‘ì¤‘ë„ > 0.99 AND ê±°ë˜ëŸ‰ > ì¤‘ì•™ê°’ (~340,000ê±´, 38%)
5. ê±°ëŒ€í˜• ê³ ë˜: ê±°ë˜ëŸ‰ > ìƒìœ„ 1% (8,890ê±´, 1%)

ì£¼ìš” ì‘ì—…:
1. ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ë¼ë²¨ë§
2. ìì—°ìŠ¤ëŸ¬ìš´ í´ë˜ìŠ¤ ë¶ˆê· í˜• ìƒí™© ìƒì„±
3. Class Weight ì¡°ì • (4ê°€ì§€ ì „ëµ) ë¹„êµ
4. Random Forest ëª¨ë¸ ì„±ëŠ¥ ê°œì„  ë¶„ì„
5. ìµœì  ë°ì´í„°ì…‹ ì¤€ë¹„

ì˜ˆìƒ ê²°ê³¼: Random Forestìš© ìµœì í™”ëœ ê±°ë˜ í–‰ë™ íŒ¨í„´ ë¶„ë¥˜ ë°ì´í„°ì…‹
"""

import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
import matplotlib.pyplot as plt
import seaborn as sns
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['AppleGothic', 'Malgun Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class WhaleDetectionAnalyzer:
    def __init__(self, results_dir='analysis/step1_results/class_weight_results'):
        self.results_dir = results_dir
        os.makedirs(results_dir, exist_ok=True)
        
        # ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜ ì²´ê³„
        self.whale_types = {
            'collector': 0,    # ìˆ˜ì§‘í˜• ê³ ë˜ - Input ë§ê³  Output ì ìŒ
            'distributor': 1,  # ë¶„ì‚°í˜• ê³ ë˜ - Output ë§ê³  ë¶„ì‚°
            'express': 2,      # ê¸‰í–‰í˜• ê³ ë˜ - ë†’ì€ ìˆ˜ìˆ˜ë£Œ
            'focused': 3,      # ì§‘ì¤‘í˜• ê³ ë˜ - ë†’ì€ ì§‘ì¤‘ë„
            'mega': 4          # ê±°ëŒ€í˜• ê³ ë˜ - ê·¹ëŒ€ ê±°ë˜ëŸ‰
        }
        
        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘ (ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´)
        self.class_names = {
            0: 'ìˆ˜ì§‘í˜•ê³ ë˜',    # Input > ìƒìœ„ 20% AND Output â‰¤ 2ê°œ
            1: 'ë¶„ì‚°í˜•ê³ ë˜',    # Output > ìƒìœ„ 15% AND ì§‘ì¤‘ë„ < 0.8  
            2: 'ê¸‰í–‰í˜•ê³ ë˜',    # Fee > ìƒìœ„ 5%
            3: 'ì§‘ì¤‘í˜•ê³ ë˜',    # ì§‘ì¤‘ë„ > 0.99 AND ê±°ë˜ëŸ‰ > ì¤‘ì•™ê°’
            4: 'ê±°ëŒ€í˜•ê³ ë˜'     # ê±°ë˜ëŸ‰ > ìƒìœ„ 1%
        }
        
        print("ğŸ‹ ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜ ë¶„ì„ ë„êµ¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ê²½ë¡œ: {results_dir}")
        print("ğŸ“Š ë¶„ì„ ëŒ€ìƒ: 1000 BTC ì´ìƒ ëŒ€í˜• ê±°ë˜ì˜ í–‰ë™ íŒ¨í„´ ë¶„ë¥˜")
        print("ğŸ¯ ëª©í‘œ: ìˆ˜ì§‘í˜•, ë¶„ì‚°í˜•, ê¸‰í–‰í˜•, ì§‘ì¤‘í˜•, ê±°ëŒ€í˜• ê³ ë˜ êµ¬ë¶„")
    
    def load_and_label_data(self, data_path='data/1000btc.csv'):
        """1000 BTC ì´ìƒ ëŒ€í˜• ê±°ë˜ ë°ì´í„° ë¡œë“œ ë° ì‹¤ì œ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜"""
        print("ğŸ“Š ëŒ€í˜• ê±°ë˜ ë°ì´í„° ë¡œë”© ë° í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜ ì¤‘...")
        
        # ì›ë³¸ ë°ì´í„° ë¡œë“œ
        df = pd.read_csv(data_path)
        print(f"âœ… ëŒ€í˜• ê±°ë˜ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(df):,}ê±´")
        
        # BTC ë‹¨ìœ„ë¡œ ë³€í™˜ (satoshi â†’ BTC)
        df['total_output_btc'] = df['total_output_value'] / 100000000
        df['total_input_btc'] = df['total_input_value'] / 100000000  
        df['fee_btc'] = df['fee'] / 100000000
        df['max_output_btc'] = df['max_output_value'] / 100000000
        
        # ì´ ê±°ë˜ëŸ‰ (Inputê³¼ Output ì¤‘ ìµœëŒ€ê°’)
        df['total_volume_btc'] = df[['total_input_btc', 'total_output_btc']].max(axis=1)
        
        # ì§‘ì¤‘ë„ ê³„ì‚° (ì´ë¯¸ ìˆëŠ” max_output_ratio í™œìš©)
        df['concentration'] = df['max_output_ratio']
        
        # ë¶„ë¥˜ ê¸°ì¤€ ì„ê³„ê°’ ê³„ì‚°
        volume_p99 = df['total_volume_btc'].quantile(0.99)  # ìƒìœ„ 1%
        volume_median = df['total_volume_btc'].median()     # ì¤‘ì•™ê°’
        input_p80 = df['input_count'].quantile(0.80)       # ìƒìœ„ 20%
        output_p85 = df['output_count'].quantile(0.85)     # ìƒìœ„ 15%
        fee_p95 = df['fee_btc'].quantile(0.95)             # ìƒìœ„ 5%
        
        print(f"ğŸ“Š ë¶„ë¥˜ ê¸°ì¤€ ì„ê³„ê°’:")
        print(f"  ê±°ë˜ëŸ‰ ìƒìœ„ 1%: {volume_p99:.0f} BTC")
        print(f"  ê±°ë˜ëŸ‰ ì¤‘ì•™ê°’: {volume_median:.0f} BTC")
        print(f"  Input ìƒìœ„ 20%: {input_p80:.0f}ê°œ")
        print(f"  Output ìƒìœ„ 15%: {output_p85:.0f}ê°œ")
        print(f"  ìˆ˜ìˆ˜ë£Œ ìƒìœ„ 5%: {fee_p95:.4f} BTC")
        
        # ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ë¶„ë¥˜
        def classify_whale_behavior(row):
            # ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ë¶„ë¥˜ (ê²¹ì¹˜ëŠ” ê²½ìš° ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê²ƒìœ¼ë¡œ ë¶„ë¥˜)
            
            # 1. ê±°ëŒ€í˜• ê³ ë˜ (ìµœìš°ì„ ) - ê±°ë˜ëŸ‰ > ìƒìœ„ 1%
            if row['total_volume_btc'] >= volume_p99:
                return 4  # ê±°ëŒ€í˜• ê³ ë˜
            
            # 2. ê¸‰í–‰í˜• ê³ ë˜ - Fee > ìƒìœ„ 5%
            elif row['fee_btc'] >= fee_p95:
                return 2  # ê¸‰í–‰í˜• ê³ ë˜
            
            # 3. ìˆ˜ì§‘í˜• ê³ ë˜ - Input > ìƒìœ„ 20% AND Output â‰¤ 2ê°œ
            elif row['input_count'] >= input_p80 and row['output_count'] <= 2:
                return 0  # ìˆ˜ì§‘í˜• ê³ ë˜
            
            # 4. ë¶„ì‚°í˜• ê³ ë˜ - Output > ìƒìœ„ 15% AND ì§‘ì¤‘ë„ < 0.8
            elif row['output_count'] >= output_p85 and row['concentration'] < 0.8:
                return 1  # ë¶„ì‚°í˜• ê³ ë˜
            
            # 5. ì§‘ì¤‘í˜• ê³ ë˜ - ì§‘ì¤‘ë„ > 0.99 AND ê±°ë˜ëŸ‰ > ì¤‘ì•™ê°’
            elif row['concentration'] > 0.99 and row['total_volume_btc'] > volume_median:
                return 3  # ì§‘ì¤‘í˜• ê³ ë˜
            
            # 6. ê¸°ë³¸ê°’ - ì§‘ì¤‘í˜• ê³ ë˜ (ë‚˜ë¨¸ì§€)
            else:
                return 3  # ì§‘ì¤‘í˜• ê³ ë˜ (ê¸°ë³¸)
        
        # ë¼ë²¨ë§ ì ìš©
        df['whale_class'] = df.apply(classify_whale_behavior, axis=1)
        
        # í”¼ì²˜ ì„ íƒ (í–‰ë™ íŒ¨í„´ ë¶„ì„ì— ì í•©í•œ í”¼ì²˜ë“¤)
        feature_cols = [
            'total_volume_btc',    # ì´ ê±°ë˜ëŸ‰ (í•µì‹¬ ì§€í‘œ)
            'input_count',         # ì…ë ¥ ê°œìˆ˜ (ìˆ˜ì§‘ íŒ¨í„´)
            'output_count',        # ì¶œë ¥ ê°œìˆ˜ (ë¶„ì‚° íŒ¨í„´)  
            'concentration',       # ì§‘ì¤‘ë„ (max_output_ratio)
            'fee_btc'              # ìˆ˜ìˆ˜ë£Œ (ê¸‰í–‰ íŒ¨í„´)
        ]
        
        # ë°ì´í„° ì •ë¦¬
        X = df[feature_cols].copy()
        y = df['whale_class'].copy()
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        X = X.fillna(0)
        
        # ê·¹ë‹¨ê°’ ì²˜ë¦¬ (IQR ë°©ì‹)
        for col in X.columns:
            if X[col].dtype in ['float64', 'int64']:
                Q1 = X[col].quantile(0.25)
                Q3 = X[col].quantile(0.75)
                IQR = Q3 - Q1
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                X[col] = X[col].clip(lower_bound, upper_bound)
        
        print(f"ğŸ“Š ì„ íƒëœ í”¼ì²˜: {feature_cols}")
        print(f"ğŸ·ï¸ í´ë˜ìŠ¤ ìˆ˜: {len(y.unique())}ê°œ")
        
        # ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ë¶„í¬ ì¶œë ¥
        class_dist = y.value_counts().sort_index()
        total = len(y)
        print("\nğŸ“Š ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ë¶„í¬:")
        for cls, count in class_dist.items():
            percentage = (count / total) * 100
            print(f"  í´ë˜ìŠ¤ {cls} ({self.class_names[cls]}): {count:,}ê±´ ({percentage:.1f}%)")
        
        # ê° í´ë˜ìŠ¤ë³„ íŠ¹ì„± í™•ì¸
        print("\nğŸ’¼ ê° í´ë˜ìŠ¤ë³„ ì‹¤ì œ íŠ¹ì„±:")
        for cls in sorted(y.unique()):
            mask = y == cls
            cls_data = df[mask]
            avg_volume = cls_data['total_volume_btc'].mean()
            avg_input = cls_data['input_count'].mean()
            avg_output = cls_data['output_count'].mean()
            avg_concentration = cls_data['concentration'].mean()
            avg_fee = cls_data['fee_btc'].mean()
            
            print(f"  í´ë˜ìŠ¤ {cls} ({self.class_names[cls]}):")
            print(f"    í‰ê·  ê±°ë˜ëŸ‰: {avg_volume:.0f} BTC")
            print(f"    í‰ê·  Input: {avg_input:.1f}ê°œ")
            print(f"    í‰ê·  Output: {avg_output:.1f}ê°œ")
            print(f"    í‰ê·  ì§‘ì¤‘ë„: {avg_concentration:.3f}")
            print(f"    í‰ê·  ìˆ˜ìˆ˜ë£Œ: {avg_fee:.4f} BTC")
        
        # í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨ ê³„ì‚°
        max_class = class_dist.max()
        min_class = class_dist.min()
        imbalance_ratio = max_class / min_class
        print(f"\nâš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.1f}:1")
        
        return X, y
    
    def calculate_class_weights(self, y):
        """ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜ì— íŠ¹í™”ëœ class weight ì „ëµ ê³„ì‚°"""
        print("\nâš–ï¸ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ Class Weight ì „ëµ ê³„ì‚° ì¤‘...")
        
        classes = np.unique(y)
        
        # 1. Balanced (sklearn ê¸°ë³¸)
        balanced_weights = compute_class_weight('balanced', classes=classes, y=y)
        balanced_dict = dict(zip(classes, balanced_weights))
        
        # 2. Behavior-Focused (í–‰ë™ íŒ¨í„´ ì¤‘ì‹¬)
        # íŠ¹ìˆ˜í•œ í–‰ë™ íŒ¨í„´(ìˆ˜ì§‘í˜•, ë¶„ì‚°í˜•, ê¸‰í–‰í˜•, ê±°ëŒ€í˜•)ì— ë†’ì€ ê°€ì¤‘ì¹˜
        behavior_dict = {}
        class_counts = np.bincount(y)
        total_samples = len(y)
        
        for cls in classes:
            base_weight = total_samples / (len(classes) * class_counts[cls])
            if cls == 4:  # ê±°ëŒ€í˜• ê³ ë˜ (ê·¹í¬ê·€)
                behavior_dict[cls] = base_weight * 50  # ìµœê³  ê°€ì¤‘ì¹˜
            elif cls == 2:  # ê¸‰í–‰í˜• ê³ ë˜ (ë†’ì€ ìˆ˜ìˆ˜ë£Œ)
                behavior_dict[cls] = base_weight * 20  # ë†’ì€ ê°€ì¤‘ì¹˜
            elif cls == 0:  # ìˆ˜ì§‘í˜• ê³ ë˜ (íŠ¹ìˆ˜ íŒ¨í„´)
                behavior_dict[cls] = base_weight * 10
            elif cls == 1:  # ë¶„ì‚°í˜• ê³ ë˜ (ë¶„ì‚° íŒ¨í„´)
                behavior_dict[cls] = base_weight * 5
            else:  # ì§‘ì¤‘í˜• ê³ ë˜ (ì¼ë°˜ì )
                behavior_dict[cls] = base_weight * 1
        
        # 3. Rarity-Based (í¬ê·€ë„ ê¸°ë°˜)
        # í´ë˜ìŠ¤ë³„ í¬ê·€ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ ì¡°ì •
        rarity_dict = {}
        for cls in classes:
            count = class_counts[cls]
            percentage = (count / total_samples) * 100
            
            if percentage < 2:  # 2% ë¯¸ë§Œ - ê·¹í¬ê·€
                rarity_dict[cls] = 30.0
            elif percentage < 5:  # 5% ë¯¸ë§Œ - ë§¤ìš° í¬ê·€
                rarity_dict[cls] = 15.0
            elif percentage < 15:  # 15% ë¯¸ë§Œ - í¬ê·€
                rarity_dict[cls] = 5.0
            elif percentage < 25:  # 25% ë¯¸ë§Œ - ì ìŒ
                rarity_dict[cls] = 2.0
            else:  # 25% ì´ìƒ - ì¼ë°˜ì 
                rarity_dict[cls] = 0.8
        
        # 4. Business-Priority (ë¹„ì¦ˆë‹ˆìŠ¤ ìš°ì„ ìˆœìœ„)
        # ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ ì¤‘ìš”ë„ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜
        business_dict = {}
        base_weights = compute_class_weight('balanced', classes=classes, y=y)
        
        priority_multipliers = {
            4: 25.0,  # ê±°ëŒ€í˜• ê³ ë˜ - ìµœê³  ìš°ì„ ìˆœìœ„ (ì‹œì¥ ì˜í–¥ë ¥)
            2: 15.0,  # ê¸‰í–‰í˜• ê³ ë˜ - ë†’ì€ ìš°ì„ ìˆœìœ„ (ê¸´ê¸‰ì„±)
            0: 8.0,   # ìˆ˜ì§‘í˜• ê³ ë˜ - ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (ì¶•ì  íŒ¨í„´)
            1: 5.0,   # ë¶„ì‚°í˜• ê³ ë˜ - ì¤‘ê°„ ìš°ì„ ìˆœìœ„ (ìœ í†µ íŒ¨í„´)
            3: 2.0    # ì§‘ì¤‘í˜• ê³ ë˜ - ë‚®ì€ ìš°ì„ ìˆœìœ„ (ì¼ë°˜ì )
        }
        
        for i, cls in enumerate(classes):
            business_dict[cls] = base_weights[i] * priority_multipliers.get(cls, 1.0)
        
        strategies = {
            'balanced': balanced_dict,
            'behavior_focused': behavior_dict,
            'rarity_based': rarity_dict,
            'business_priority': business_dict
        }
        
        # ì „ëµ ì¶œë ¥
        for name, weights in strategies.items():
            print(f"\nğŸ¯ {name.title().replace('_', ' ')} ì „ëµ:")
            for cls, weight in weights.items():
                print(f"  í´ë˜ìŠ¤ {cls} ({self.class_names[cls]}): {weight:.2f}")
        
        return strategies
    
    def evaluate_model_with_cv(self, X, y, class_weight=None, strategy_name="Baseline"):
        """í˜„ì‹¤ì ì¸ ê³ ë˜ ê±°ë˜ íƒì§€ ëª¨ë¸ í‰ê°€ (ì§„í–‰ë¥  í‘œì‹œ í¬í•¨)"""
        print(f"\nğŸ”„ {strategy_name} ì „ëµ í‰ê°€ ì‹œì‘...")
        
        # í‘œì¤€í™” (5% ì™„ë£Œ)
        print(f"  â³ ë°ì´í„° í‘œì¤€í™” ì¤‘... (5%)")
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        # ê³ ë˜ íƒì§€ì— ì í•©í•œ ëª¨ë¸ ì„¤ì • (10% ì™„ë£Œ)
        print(f"  â³ Random Forest ëª¨ë¸ ì„¤ì • ì¤‘... (10%)")
        model = RandomForestClassifier(
            n_estimators=100,       # ì¶©ë¶„í•œ íŠ¸ë¦¬ ìˆ˜
            random_state=42,
            class_weight=class_weight,
            max_depth=8,           # ì ì ˆí•œ ê¹Šì´
            min_samples_split=50,   # ê³¼ì í•© ë°©ì§€
            min_samples_leaf=20,    # ë¦¬í”„ ë…¸ë“œ ìµœì†Œ ìƒ˜í”Œ
            max_features='sqrt',    # í”¼ì²˜ ìƒ˜í”Œë§
            bootstrap=True,
            oob_score=True
        )
        
        # 5-Fold ê³„ì¸µí™” êµì°¨ ê²€ì¦ (10% -> 70% ì™„ë£Œ)
        print(f"  â³ 5-Fold êµì°¨ ê²€ì¦ ì‹¤í–‰ ì¤‘...")
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        
        # êµì°¨ ê²€ì¦ì„ ë‹¨ê³„ë³„ë¡œ ì‹¤í–‰í•˜ì—¬ ì§„í–‰ë¥  í‘œì‹œ
        cv_scores = []
        fold_progress = [20, 30, 45, 60, 70]  # ê° foldë³„ ì§„í–‰ë¥ 
        
        for i, (train_idx, val_idx) in enumerate(cv.split(X_scaled, y)):
            print(f"    ğŸ“Š Fold {i+1}/5 ì²˜ë¦¬ ì¤‘... ({fold_progress[i]}%)")
            X_train_fold, X_val_fold = X_scaled[train_idx], X_scaled[val_idx]
            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]
            
            # ëª¨ë¸ í›ˆë ¨ ë° ì˜ˆì¸¡
            model_fold = RandomForestClassifier(
                n_estimators=100, random_state=42, class_weight=class_weight,
                max_depth=8, min_samples_split=50, min_samples_leaf=20,
                max_features='sqrt', bootstrap=True
            )
            model_fold.fit(X_train_fold, y_train_fold)
            y_pred_fold = model_fold.predict(X_val_fold)
            
            # F1-Score ê³„ì‚°
            fold_f1 = f1_score(y_val_fold, y_pred_fold, average='macro')
            cv_scores.append(fold_f1)
        
        cv_scores = np.array(cv_scores)
        
        # í™€ë“œì•„ì›ƒ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ë¡œ ìµœì¢… í‰ê°€ (70% -> 90% ì™„ë£Œ)
        print(f"  â³ í™€ë“œì•„ì›ƒ í…ŒìŠ¤íŠ¸ í‰ê°€ ì¤‘... (80%)")
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y, test_size=0.3, random_state=42, stratify=y
        )
        
        print(f"  â³ ìµœì¢… ëª¨ë¸ í›ˆë ¨ ì¤‘... (85%)")
        model.fit(X_train, y_train)
        
        print(f"  â³ ìµœì¢… ì˜ˆì¸¡ ë° í‰ê°€ ì¤‘... (90%)")
        y_pred = model.predict(X_test)
        
        # ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œ ê³„ì‚° (90% -> 100% ì™„ë£Œ)
        print(f"  â³ í‰ê°€ ì§€í‘œ ê³„ì‚° ì¤‘... (95%)")
        f1_macro = f1_score(y_test, y_pred, average='macro')
        f1_weighted = f1_score(y_test, y_pred, average='weighted')
        
        # ê³ ë˜ í´ë˜ìŠ¤ ì „ìš© F1-Score (í´ë˜ìŠ¤ 1,2,3,4)
        whale_mask = y_test > 0
        if whale_mask.sum() > 0:
            whale_f1 = f1_score(y_test[whale_mask], y_pred[whale_mask], average='macro')
        else:
            whale_f1 = 0.0
        
        # ìƒì„¸ ë¦¬í¬íŠ¸
        report = classification_report(y_test, y_pred, output_dict=True)
        
        print(f"  âœ… {strategy_name} í‰ê°€ ì™„ë£Œ! (100%)")
        print(f"ğŸ“Š {strategy_name}:")
        print(f"  êµì°¨ê²€ì¦ F1-Macro: {cv_scores.mean():.4f} (Â±{cv_scores.std()*2:.4f})")
        print(f"  í…ŒìŠ¤íŠ¸ F1-Macro: {f1_macro:.4f}")
        print(f"  ê³ ë˜ ì „ìš© F1-Score: {whale_f1:.4f}")
        print(f"  OOB Score: {model.oob_score_:.4f}")
        
        return {
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'test_f1_macro': f1_macro,
            'test_f1_weighted': f1_weighted,
            'whale_f1': whale_f1,
            'oob_score': model.oob_score_,
            'cv_scores': cv_scores,
            'classification_report': report,
            'confusion_matrix': confusion_matrix(y_test, y_pred),
            'y_test': y_test,
            'y_pred': y_pred,
            'feature_importance': dict(zip(X.columns, model.feature_importances_))
        }
    
    def run_analysis(self, data_path='data/1000btc.csv'):
        """ì „ì²´ ë¶„ì„ ì‹¤í–‰ (ì§„í–‰ë¥  í‘œì‹œ í¬í•¨)"""
        print("ğŸš€ ì‹¤ì œ ê±°ë˜ í–‰ë™ íŒ¨í„´ ê¸°ë°˜ ê³ ë˜ ë¶„ë¥˜ ë¶„ì„ì„ ì‹œì‘í•©ë‹ˆë‹¤!")
        print("=" * 60)
        print("ğŸ“Š ë¶„ì„ ë²”ìœ„: 1000 BTC ì´ìƒ ëŒ€í˜• ê±°ë˜ì˜ ì‹¤ì œ í–‰ë™ íŒ¨í„´ ë¶„ë¥˜")
        print("ğŸ¯ ëª©í‘œ: ìˆ˜ì§‘í˜•, ë¶„ì‚°í˜•, ê¸‰í–‰í˜•, ì§‘ì¤‘í˜•, ê±°ëŒ€í˜• ê³ ë˜ êµ¬ë¶„")
        print("ğŸŒ³ ëª¨ë¸: Random Forest ê¸°ë°˜ ë¶„ë¥˜")
        print("âš–ï¸ ëª©ì : Class Weight ì¡°ì •ì„ í†µí•œ ìµœì  ë°ì´í„°ì…‹ ì¤€ë¹„")
        
        # ë°ì´í„° ì¤€ë¹„ (0% -> 20%)
        print(f"\nâ³ ì „ì²´ ì§„í–‰ë¥ : 0% - ë°ì´í„° ë¡œë”© ë° ë¼ë²¨ë§ ì‹œì‘...")
        X, y = self.load_and_label_data(data_path)
        print(f"âœ… ì „ì²´ ì§„í–‰ë¥ : 20% - ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ!")
        
        # Class Weight ì „ëµ ê³„ì‚° (20% -> 25%)
        print(f"\nâ³ ì „ì²´ ì§„í–‰ë¥ : 20% - Class Weight ì „ëµ ê³„ì‚° ì¤‘...")
        strategies = self.calculate_class_weights(y)
        print(f"âœ… ì „ì²´ ì§„í–‰ë¥ : 25% - Class Weight ì „ëµ ê³„ì‚° ì™„ë£Œ!")
        
        # ê²°ê³¼ ì €ì¥
        results = {}
        
        print(f"\nğŸŒ³ ê±°ë˜ í–‰ë™ íŒ¨í„´ ë¶„ë¥˜ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ (êµì°¨ ê²€ì¦ í¬í•¨):")
        print("=" * 50)
        
        total_strategies = 5  # baseline + 4ê°œ ì „ëµ
        current_strategy = 0
        
        # 1. Baseline (ê°€ì¤‘ì¹˜ ì—†ìŒ) (25% -> 40%)
        current_strategy += 1
        progress = 25 + (current_strategy / total_strategies) * 60  # 25%ì—ì„œ 85%ê¹Œì§€
        print(f"\nğŸ”„ ì „ì²´ ì§„í–‰ë¥ : {progress:.0f}% - Baseline ì „ëµ í‰ê°€ ì¤‘... ({current_strategy}/{total_strategies})")
        baseline_result = self.evaluate_model_with_cv(X, y, None, "Baseline")
        results['baseline'] = baseline_result
        print(f"âœ… Baseline ì „ëµ ì™„ë£Œ!")
        
        # 2. ê° ì „ëµë³„ í‰ê°€ (40% -> 85%)
        for i, (strategy_name, class_weights) in enumerate(strategies.items()):
            current_strategy += 1
            progress = 25 + (current_strategy / total_strategies) * 60
            strategy_display_name = strategy_name.title().replace('_', ' ')
            print(f"\nğŸ”„ ì „ì²´ ì§„í–‰ë¥ : {progress:.0f}% - {strategy_display_name} ì „ëµ í‰ê°€ ì¤‘... ({current_strategy}/{total_strategies})")
            result = self.evaluate_model_with_cv(X, y, class_weights, strategy_display_name)
            results[strategy_name] = result
            print(f"âœ… {strategy_display_name} ì „ëµ ì™„ë£Œ!")
        
        # ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™” (85% -> 100%)
        print(f"\nâ³ ì „ì²´ ì§„í–‰ë¥ : 85% - ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™” ì¤‘...")
        print(f"  ğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥ ì¤‘... (90%)")
        self.save_results(results, strategies, X, y)
        print(f"  ğŸ“Š ì‹œê°í™” ìƒì„± ì¤‘... (95%)")
        self.create_visualizations(results)
        
        # ìµœì í™”ëœ ë°ì´í„°ì…‹ ì €ì¥ (95% -> 100%)
        best_strategy = max(results.keys(), key=lambda k: results[k]['test_f1_macro'])
        print(f"  ğŸ’¾ ìµœì í™”ëœ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘... (98%)")
        dataset_files = self.save_optimized_dataset(X, y, best_strategy, strategies, results)
        
        print(f"âœ… ì „ì²´ ì§„í–‰ë¥ : 100% - ëª¨ë“  ë¶„ì„ ì™„ë£Œ! ğŸ‰")
        
        # ìµœì¢… ìš”ì•½
        best_strategy = max(results.keys(), key=lambda k: results[k]['test_f1_macro'])
        best_f1 = results[best_strategy]['test_f1_macro']
        baseline_f1 = results['baseline']['test_f1_macro']
        improvement = ((best_f1 - baseline_f1) / baseline_f1) * 100
        
        print(f"\nğŸ† ìµœì  ì „ëµ: {best_strategy.replace('_', ' ').title()}")
        print(f"ğŸ“Š ìµœê³  F1-Score: {best_f1:.4f}")
        print(f"ğŸ“ˆ ê°œì„ ìœ¨: {improvement:+.1f}%")
        print(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {self.results_dir}/")
        
        # ì €ì¥ëœ ë°ì´í„°ì…‹ íŒŒì¼ ì •ë³´ ì¶œë ¥
        print(f"\nğŸ’¾ ì €ì¥ëœ ìµœì í™” ë°ì´í„°ì…‹:")
        print(f"  ğŸ“Š ì „ì²´ ë°ì´í„°ì…‹: {dataset_files['dataset_file']}")
        print(f"  ğŸ¯ í›ˆë ¨ ë°ì´í„°ì…‹: {dataset_files['train_file']}")
        print(f"  ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹: {dataset_files['test_file']}")
        print(f"  âš™ï¸ ìµœì  ì„¤ì •: {dataset_files['config_file']}")
        print(f"  ğŸ“ ìŠ¤ì¼€ì¼ëŸ¬: {dataset_files['scaler_file']}")
        print(f"  ğŸ“– ì‚¬ìš© ê°€ì´ë“œ: {dataset_files['guide_file']}")
        
        return results
    
    def create_visualizations(self, results):
        """í–¥ìƒëœ ì‹œê°í™” ìƒì„±"""
        # 1. ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ (2x3 ë ˆì´ì•„ì›ƒ)
        fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(2, 3, figsize=(18, 12))
        
        strategies = list(results.keys())
        
        # êµì°¨ ê²€ì¦ F1-Score ë¹„êµ
        cv_means = [results[s]['cv_mean'] for s in strategies]
        cv_stds = [results[s]['cv_std'] for s in strategies]
        
        bars1 = ax1.bar(strategies, cv_means, yerr=cv_stds, capsize=5, 
                       color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
        ax1.set_title('êµì°¨ê²€ì¦ F1-Macro ë¹„êµ\n(ì—ëŸ¬ë°”: Â±2Ïƒ)', fontsize=12, fontweight='bold')
        ax1.set_ylabel('F1-Score')
        ax1.set_ylim(0, 1.0)
        ax1.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, mean, std in zip(bars1, cv_means, cv_stds):
            height = bar.get_height()
            ax1.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,
                    f'{mean:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # í…ŒìŠ¤íŠ¸ F1-Macro ë¹„êµ  
        test_f1s = [results[s]['test_f1_macro'] for s in strategies]
        bars2 = ax2.bar(strategies, test_f1s, 
                       color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
        ax2.set_title('í…ŒìŠ¤íŠ¸ F1-Macro ë¹„êµ', fontsize=12, fontweight='bold')
        ax2.set_ylabel('F1-Score')
        ax2.set_ylim(0, 1.0)
        ax2.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, f1 in zip(bars2, test_f1s):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{f1:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # OOB Score ë¹„êµ
        oob_scores = [results[s]['oob_score'] for s in strategies]
        bars3 = ax3.bar(strategies, oob_scores, 
                       color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
        ax3.set_title('OOB Score ë¹„êµ\n(ê³¼ì í•© ê²€ì¦)', fontsize=12, fontweight='bold')
        ax3.set_ylabel('OOB Score')
        ax3.set_ylim(0, 1.0)
        ax3.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, score in zip(bars3, oob_scores):
            height = bar.get_height()
            ax3.text(bar.get_x() + bar.get_width()/2., height + 0.01,
                    f'{score:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # êµì°¨ ê²€ì¦ í‘œì¤€í¸ì°¨ (ì•ˆì •ì„±)
        ax4.bar(strategies, cv_stds, color=['lightblue', 'lightgreen', 'lightcoral', 'lightyellow', 'lightpink'])
        ax4.set_title('êµì°¨ê²€ì¦ í‘œì¤€í¸ì°¨\n(ë‚®ì„ìˆ˜ë¡ ì•ˆì •ì )', fontsize=12, fontweight='bold')
        ax4.set_ylabel('í‘œì¤€í¸ì°¨')
        ax4.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for i, (strategy, std) in enumerate(zip(strategies, cv_stds)):
            ax4.text(i, std + max(cv_stds)*0.02, f'{std:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=10)
        
        # ì„±ëŠ¥ ê°œì„ ìœ¨
        baseline_f1 = results['baseline']['test_f1_macro']
        improvements = [(results[s]['test_f1_macro'] - baseline_f1) * 100 for s in strategies[1:]]
        strategy_names = [s.replace('_', ' ').title() for s in strategies[1:]]
        
        colors = ['green' if imp > 0 else 'red' if imp < 0 else 'gray' for imp in improvements]
        bars5 = ax5.bar(strategy_names, improvements, color=colors)
        ax5.set_title('Baseline ëŒ€ë¹„ ê°œì„ ìœ¨ (%)', fontsize=12, fontweight='bold')
        ax5.set_ylabel('ê°œì„ ìœ¨ (%)')
        ax5.axhline(y=0, color='black', linestyle='-', alpha=0.3)
        ax5.tick_params(axis='x', rotation=45)
        
        # ê°’ í‘œì‹œ
        for bar, imp in zip(bars5, improvements):
            height = bar.get_height()
            va = 'bottom' if height >= 0 else 'top'
            y_pos = height + max(abs(max(improvements)), abs(min(improvements))) * 0.05 if height >= 0 else height - max(abs(max(improvements)), abs(min(improvements))) * 0.05
            ax5.text(bar.get_x() + bar.get_width()/2., y_pos,
                    f'{imp:+.1f}%', ha='center', va=va, fontweight='bold', fontsize=10)
        
        # í”¼ì²˜ ì¤‘ìš”ë„ (baseline ëª¨ë¸ ê¸°ì¤€)
        feature_importance = results['baseline']['feature_importance']
        features = list(feature_importance.keys())
        importances = list(feature_importance.values())
        
        bars6 = ax6.barh(features, importances, color='skyblue')
        ax6.set_title('í”¼ì²˜ ì¤‘ìš”ë„\n(Baseline ëª¨ë¸)', fontsize=12, fontweight='bold')
        ax6.set_xlabel('ì¤‘ìš”ë„')
        
        # ê°’ í‘œì‹œ
        for bar, imp in zip(bars6, importances):
            width = bar.get_width()
            ax6.text(width + max(importances)*0.01, bar.get_y() + bar.get_height()/2.,
                    f'{imp:.3f}', ha='left', va='center', fontweight='bold', fontsize=10)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        # 2. í˜¼ë™ í–‰ë ¬ ì‹œê°í™”
        fig, axes = plt.subplots(2, 3, figsize=(18, 12))
        axes = axes.flatten()
        
        for i, (strategy, result) in enumerate(results.items()):
            if i >= 6:  # ìµœëŒ€ 6ê°œë§Œ í‘œì‹œ
                break
                
            cm = result['confusion_matrix']
            
            # ì •ê·œí™”ëœ í˜¼ë™ í–‰ë ¬
            cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
            
            im = axes[i].imshow(cm_normalized, interpolation='nearest', cmap='Blues')
            axes[i].set_title(f'{strategy.title().replace("_", " ")}\ní˜¼ë™ í–‰ë ¬', fontsize=12)
            
            # í´ë˜ìŠ¤ ë¼ë²¨
            class_labels = [f'{j}\n({self.class_names[j]})' for j in range(len(cm))]
            axes[i].set_xticks(range(len(cm)))
            axes[i].set_yticks(range(len(cm)))
            axes[i].set_xticklabels(class_labels, rotation=45, ha='right')
            axes[i].set_yticklabels(class_labels)
            
            # ê°’ í‘œì‹œ
            for j in range(len(cm)):
                for k in range(len(cm)):
                    text_color = 'white' if cm_normalized[j, k] > 0.5 else 'black'
                    axes[i].text(k, j, f'{cm_normalized[j, k]:.2f}',
                               ha='center', va='center', color=text_color, fontweight='bold')
            
            axes[i].set_ylabel('ì‹¤ì œ í´ë˜ìŠ¤')
            axes[i].set_xlabel('ì˜ˆì¸¡ í´ë˜ìŠ¤')
        
        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
        for i in range(len(results), 6):
            axes[i].set_visible(False)
        
        plt.tight_layout()
        plt.savefig(f'{self.results_dir}/confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"ğŸ“Š ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ì €ì¥: {self.results_dir}/performance_comparison.png")
        print(f"ğŸ“Š í˜¼ë™ í–‰ë ¬ ì €ì¥: {self.results_dir}/confusion_matrices.png")

    def save_results(self, results, strategies, X, y):
        """ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ì €ì¥"""
        print("\nğŸ’¾ ìƒì„¸ ê²°ê³¼ ì €ì¥ ì¤‘...")
        
        results_file = f"{self.results_dir}/detailed_analysis_results.txt"
        
        # ë°ì´í„° ë¶„í•  ì •ë³´ ê³„ì‚°
        total_samples = len(X)
        train_samples = int(total_samples * 0.7)
        test_samples = total_samples - train_samples
        
        with open(results_file, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("ğŸŒ³ ê³ ë˜ ê±°ë˜ íƒì§€ ë¶„ì„ - ìƒì„¸ ë¶„ì„ ê²°ê³¼\n")
            f.write("="*80 + "\n")
            f.write(f"ğŸ“… ë¶„ì„ ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"ğŸ“Š ì´ ìƒ˜í”Œ ìˆ˜: {total_samples:,}ê±´\n")
            f.write(f"ğŸ¯ í›ˆë ¨ ë°ì´í„°: {train_samples:,}ê±´ (70%)\n")
            f.write(f"ğŸ¯ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {test_samples:,}ê±´ (30%)\n")
            f.write(f"ğŸ“ˆ í”¼ì²˜ ìˆ˜: {len(X.columns)}ê°œ\n")
            f.write(f"ğŸ“ˆ ì‚¬ìš© í”¼ì²˜: {list(X.columns)}\n")
            
            # í´ë˜ìŠ¤ ë¶„í¬
            class_counts = pd.Series(y).value_counts().sort_index()
            max_class = class_counts.max()
            min_class = class_counts.min()
            imbalance_ratio = max_class / min_class
            f.write(f"âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¹„ìœ¨: {imbalance_ratio:.1f}:1\n\n")
            
            # í´ë˜ìŠ¤ ë¶„í¬ ìƒì„¸
            f.write("ğŸ“Š í´ë˜ìŠ¤ ë¶„í¬:\n")
            for class_id, count in class_counts.items():
                percentage = (count / len(X)) * 100
                class_name = self.class_names.get(class_id, f"í´ë˜ìŠ¤{class_id}")
                f.write(f"  í´ë˜ìŠ¤ {class_id} ({class_name}): {count:,}ê±´ ({percentage:.1f}%)\n")
            
            # Class Weight ì „ëµ
            f.write("\n" + "="*50 + "\n")
            f.write("âš–ï¸ ê³„ì‚°ëœ Class Weight ì „ëµ:\n")
            f.write("="*50 + "\n")
            
            for strategy, weights in strategies.items():
                f.write(f"\nğŸ¯ {strategy.title().replace('_', ' ')} ì „ëµ:\n")
                for class_id, weight in weights.items():
                    f.write(f"  í´ë˜ìŠ¤ {class_id}: {weight:.2f}\n")
            
            # ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ
            f.write("\n" + "="*50 + "\n")
            f.write("ğŸŒ³ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ:\n")
            f.write("="*50 + "\n")
            
            all_strategies = ['baseline'] + list(strategies.keys())
            all_names = ['Baseline (ê°€ì¤‘ì¹˜ ì—†ìŒ)'] + [s.title().replace('_', ' ') for s in strategies.keys()]
            
            for strategy, name in zip(all_strategies, all_names):
                result = results[strategy]
                f.write(f"\nğŸ“Š {name}:\n")
                f.write(f"  êµì°¨ê²€ì¦ F1-Macro: {result['cv_mean']:.4f} (Â±{result['cv_std']*2:.4f})\n")
                f.write(f"  í…ŒìŠ¤íŠ¸ F1-Macro: {result['test_f1_macro']:.4f}\n")
                f.write(f"  ê³ ë˜ ì „ìš© F1-Score: {result['whale_f1']:.4f}\n")
                f.write(f"  OOB Score: {result['oob_score']:.4f}\n")
                
                if strategy != 'baseline':
                    baseline_f1 = results['baseline']['test_f1_macro']
                    improvement = ((result['test_f1_macro'] - baseline_f1) / baseline_f1) * 100
                    f.write(f"  ê°œì„ ìœ¨: {improvement:+.1f}%\n")
            
            # í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„
            f.write("\n" + "="*50 + "\n")
            f.write("ğŸ“Š í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ (Baseline ëª¨ë¸):\n")
            f.write("="*50 + "\n")
            
            feature_importance = results['baseline']['feature_importance']
            sorted_features = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)
            
            for feature, importance in sorted_features:
                f.write(f"  {feature}: {importance:.4f}\n")
            
            # í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„ (ìƒìœ„ 3ê°œ ì „ëµë§Œ)
            f.write("\n" + "="*50 + "\n")
            f.write("ğŸ“ˆ í´ë˜ìŠ¤ë³„ ìƒì„¸ ì„±ëŠ¥ ë¶„ì„:\n")
            f.write("="*50 + "\n")
            
            # ìƒìœ„ 3ê°œ ì „ëµ ì„ íƒ
            strategy_f1_scores = {s: results[s]['test_f1_macro'] for s in all_strategies}
            top_strategies = sorted(strategy_f1_scores.items(), key=lambda x: x[1], reverse=True)[:3]
            
            for strategy, _ in top_strategies:
                strategy_name = 'Baseline (ê°€ì¤‘ì¹˜ ì—†ìŒ)' if strategy == 'baseline' else strategy.title().replace('_', ' ')
                f.write(f"\nğŸ¯ {strategy_name} ì „ëµ:\n")
                f.write("-" * 40 + "\n")
                
                report = results[strategy]['classification_report']
                
                for class_id in range(5):
                    class_name = self.class_names.get(class_id, f"í´ë˜ìŠ¤{class_id}")
                    
                    try:
                        class_report = report[str(class_id)]
                    except KeyError:
                        try:
                            class_report = report[class_id]
                        except KeyError:
                            continue
                    
                    precision = class_report['precision']
                    recall = class_report['recall']
                    f1_score = class_report['f1-score']
                    support = class_report['support']
                    
                    f.write(f"  í´ë˜ìŠ¤ {class_id} ({class_name}):\n")
                    f.write(f"    Precision: {precision:.4f}\n")
                    f.write(f"    Recall: {recall:.4f}\n")
                    f.write(f"    F1-Score: {f1_score:.4f}\n")
                    f.write(f"    Support: {support}\n")
            
            # ê¶Œì¥ì‚¬í•­
            f.write("\n" + "="*50 + "\n")
            f.write("ğŸ’¡ ë¶„ì„ ê²°ê³¼ ë° ê¶Œì¥ì‚¬í•­:\n")
            f.write("="*50 + "\n")
            
            # ìµœì  ì „ëµ ì°¾ê¸°
            best_strategy = max(strategy_f1_scores, key=strategy_f1_scores.get)
            best_f1 = strategy_f1_scores[best_strategy]
            baseline_f1 = strategy_f1_scores['baseline']
            overall_improvement = ((best_f1 - baseline_f1) / baseline_f1) * 100
            
            f.write(f"\nğŸ† ìµœì  ì „ëµ: {best_strategy}\n")
            f.write(f"ğŸ“Š ìµœê³  F1-Score: {best_f1:.4f}\n")
            f.write(f"ğŸ“ˆ ì „ì²´ ê°œì„ ìœ¨: {overall_improvement:+.1f}%\n")
            
            f.write(f"\nğŸ’¼ í”„ë¡œë•ì…˜ ê¶Œì¥ì‚¬í•­:\n")
            if overall_improvement > 1:
                f.write(f"1. '{best_strategy}' ì „ëµ ì‚¬ìš© ê¶Œì¥ - {overall_improvement:.1f}% ì„±ëŠ¥ ê°œì„ \n")
                f.write(f"2. ì†Œìˆ˜ í´ë˜ìŠ¤ íƒì§€ìœ¨ ê°œì„  íš¨ê³¼ í™•ì¸ë¨\n")
            else:
                f.write(f"1. Class Weight ì¡°ì •ìœ¼ë¡œëŠ” ì œí•œì  ê°œì„  ({overall_improvement:.1f}%)\n")
                f.write(f"2. ëŒ€ì•ˆ: SMOTE ì˜¤ë²„ìƒ˜í”Œë§, í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§, ì•™ìƒë¸” ê³ ë ¤\n")
            
            f.write(f"3. ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ êµì°¨ ê²€ì¦ ì§€ì† ëª¨ë‹ˆí„°ë§\n")
            f.write(f"4. ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ì— ë”°ë¥¸ í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ì¡°ì • ê³ ë ¤\n")
        
        print(f"ğŸ“„ ìƒì„¸ ê²°ê³¼ ì €ì¥: {results_file}")
        
        # ë¹ ë¥¸ ìš”ì•½ íŒŒì¼ë„ ìƒì„±
        summary_file = f"{self.results_dir}/quick_summary.txt"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write("ğŸŒ³ ê³ ë˜ ê±°ë˜ íƒì§€ ë¶„ì„ ê²°ê³¼ ìš”ì•½\n")
            f.write("="*40 + "\n")
            f.write(f"ğŸ“Š ë°ì´í„°: {len(X):,}ê±´\n")
            f.write(f"âš–ï¸ í´ë˜ìŠ¤ ë¶ˆê· í˜•: {imbalance_ratio:.1f}:1\n")
            f.write(f"ğŸ† ìµœì  ì „ëµ: {best_strategy}\n")
            f.write(f"ğŸ“ˆ F1-Score: {best_f1:.4f}\n")
            f.write(f"ğŸ“Š ê°œì„ ìœ¨: {overall_improvement:+.1f}%\n")
            f.write(f"ğŸ“ˆ ì‚¬ìš© í”¼ì²˜: {list(X.columns)}\n")
        
        print(f"ğŸ“‹ ìš”ì•½ ì €ì¥: {summary_file}")

    def save_optimized_dataset(self, X, y, best_strategy, strategies, results):
        """ìµœì  Class Weight ì „ëµì´ ì ìš©ëœ ë°ì´í„°ì…‹ ì €ì¥"""
        print(f"\nğŸ’¾ ìµœì í™”ëœ ë°ì´í„°ì…‹ ì €ì¥ ì¤‘...")
        
        # ìµœì  Class Weight ê°€ì ¸ì˜¤ê¸°
        if best_strategy == 'baseline':
            best_class_weights = None
            class_weights_dict = {}
        else:
            best_class_weights = strategies[best_strategy]
            class_weights_dict = best_class_weights
        
        # 1. ì›ë³¸ ë°ì´í„° + ë¼ë²¨ + Class Weight ì •ë³´ ì €ì¥
        dataset_df = X.copy()
        dataset_df['whale_class'] = y
        dataset_df['class_name'] = y.map(self.class_names)
        
        # Class Weight ì •ë³´ ì¶”ê°€
        if class_weights_dict:
            dataset_df['class_weight'] = y.map(class_weights_dict)
        else:
            dataset_df['class_weight'] = 1.0  # Baselineì€ ëª¨ë“  í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ 1.0
        
        # ë°ì´í„°ì…‹ ì €ì¥
        dataset_file = f"{self.results_dir}/optimized_whale_dataset.csv"
        dataset_df.to_csv(dataset_file, index=False, encoding='utf-8')
        print(f"ğŸ“Š ìµœì í™”ëœ ë°ì´í„°ì…‹ ì €ì¥: {dataset_file}")
        
        # 2. ìµœì  Class Weight ì„¤ì • íŒŒì¼ ì €ì¥
        config_file = f"{self.results_dir}/optimal_class_weights.json"
        import json
        
        config_data = {
            "optimal_strategy": best_strategy,
            "strategy_description": {
                "baseline": "ê°€ì¤‘ì¹˜ ì—†ìŒ (ëª¨ë“  í´ë˜ìŠ¤ ë™ì¼ ì²˜ë¦¬)",
                "balanced": "sklearn ê¸°ë³¸ balanced ê°€ì¤‘ì¹˜",
                "behavior_focused": "ê±°ë˜ í–‰ë™ íŒ¨í„´ ì¤‘ì‹¬ ê°€ì¤‘ì¹˜",
                "rarity_based": "í´ë˜ìŠ¤ í¬ê·€ë„ ê¸°ë°˜ ê°€ì¤‘ì¹˜",
                "business_priority": "ë¹„ì¦ˆë‹ˆìŠ¤ ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê°€ì¤‘ì¹˜"
            },
            "optimal_class_weights": {str(k): float(v) for k, v in class_weights_dict.items()},
            "performance_metrics": {
                "test_f1_macro": float(results[best_strategy]['test_f1_macro']),
                "cv_mean_f1": float(results[best_strategy]['cv_mean']),
                "cv_std_f1": float(results[best_strategy]['cv_std']),
                "oob_score": float(results[best_strategy]['oob_score'])
            },
            "class_mapping": {str(k): v for k, v in self.class_names.items()},
            "feature_columns": list(X.columns),
            "dataset_info": {
                "total_samples": int(len(X)),
                "feature_count": int(len(X.columns)),
                "class_distribution": {str(k): int(v) for k, v in y.value_counts().sort_index().to_dict().items()}
            }
        }
        
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, ensure_ascii=False, indent=2)
        
        print(f"âš™ï¸ ìµœì  ì„¤ì • ì €ì¥: {config_file}")
        
        # 3. í›ˆë ¨ìš©/í…ŒìŠ¤íŠ¸ìš© ë¶„í• ëœ ë°ì´í„°ì…‹ë„ ì €ì¥
        from sklearn.model_selection import train_test_split
        
        # í‘œì¤€í™”
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        X_scaled_df = pd.DataFrame(X_scaled, columns=X.columns)
        
        # ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled_df, y, test_size=0.3, random_state=42, stratify=y
        )
        
        # í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥
        train_df = X_train.copy()
        train_df['whale_class'] = y_train
        train_df['class_name'] = y_train.map(self.class_names)
        if class_weights_dict:
            train_df['class_weight'] = y_train.map(class_weights_dict)
        else:
            train_df['class_weight'] = 1.0
        
        train_file = f"{self.results_dir}/train_dataset_optimized.csv"
        train_df.to_csv(train_file, index=False, encoding='utf-8')
        print(f"ğŸ¯ í›ˆë ¨ ë°ì´í„°ì…‹ ì €ì¥: {train_file}")
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥  
        test_df = X_test.copy()
        test_df['whale_class'] = y_test
        test_df['class_name'] = y_test.map(self.class_names)
        if class_weights_dict:
            test_df['class_weight'] = y_test.map(class_weights_dict)
        else:
            test_df['class_weight'] = 1.0
            
        test_file = f"{self.results_dir}/test_dataset_optimized.csv"
        test_df.to_csv(test_file, index=False, encoding='utf-8')
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì €ì¥: {test_file}")
        
        # 4. ìŠ¤ì¼€ì¼ëŸ¬ë„ ì €ì¥ (ë‚˜ì¤‘ì— ìƒˆ ë°ì´í„° ì˜ˆì¸¡í•  ë•Œ ì‚¬ìš©)
        import pickle
        scaler_file = f"{self.results_dir}/feature_scaler.pkl"
        with open(scaler_file, 'wb') as f:
            pickle.dump(scaler, f)
        print(f"ğŸ“ í”¼ì²˜ ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥: {scaler_file}")
        
        # 5. ì‚¬ìš©ë²• ê°€ì´ë“œ ì €ì¥
        guide_file = f"{self.results_dir}/dataset_usage_guide.md"
        with open(guide_file, 'w', encoding='utf-8') as f:
            f.write("# ğŸŒ³ ìµœì í™”ëœ ê³ ë˜ ë¶„ë¥˜ ë°ì´í„°ì…‹ ì‚¬ìš© ê°€ì´ë“œ\n\n")
            f.write("## ğŸ“Š ì €ì¥ëœ íŒŒì¼ë“¤\n\n")
            f.write("### 1. ë°ì´í„°ì…‹ íŒŒì¼\n")
            f.write("- `optimized_whale_dataset.csv`: ì „ì²´ ìµœì í™”ëœ ë°ì´í„°ì…‹\n")
            f.write("- `train_dataset_optimized.csv`: í›ˆë ¨ìš© ë°ì´í„°ì…‹ (70%, í‘œì¤€í™” ì ìš©)\n")
            f.write("- `test_dataset_optimized.csv`: í…ŒìŠ¤íŠ¸ìš© ë°ì´í„°ì…‹ (30%, í‘œì¤€í™” ì ìš©)\n\n")
            f.write("### 2. ì„¤ì • íŒŒì¼\n")
            f.write("- `optimal_class_weights.json`: ìµœì  Class Weight ì„¤ì •\n")
            f.write("- `feature_scaler.pkl`: í”¼ì²˜ í‘œì¤€í™” ìŠ¤ì¼€ì¼ëŸ¬\n\n")
            f.write("### 3. ë¶„ì„ ê²°ê³¼\n")
            f.write("- `detailed_analysis_results.txt`: ìƒì„¸ ë¶„ì„ ê²°ê³¼\n")
            f.write("- `quick_summary.txt`: ë¹ ë¥¸ ìš”ì•½\n")
            f.write("- `performance_comparison.png`: ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸\n")
            f.write("- `confusion_matrices.png`: í˜¼ë™ í–‰ë ¬\n\n")
            f.write("## ğŸš€ ì‚¬ìš© ë°©ë²•\n\n")
            f.write("### Pythonì—ì„œ ë°ì´í„°ì…‹ ë¡œë“œ\n")
            f.write("```python\n")
            f.write("import pandas as pd\n")
            f.write("import json\n")
            f.write("import pickle\n")
            f.write("from sklearn.ensemble import RandomForestClassifier\n\n")
            f.write("# 1. í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n")
            f.write("train_df = pd.read_csv('train_dataset_optimized.csv')\n")
            f.write("X_train = train_df.drop(['whale_class', 'class_name', 'class_weight'], axis=1)\n")
            f.write("y_train = train_df['whale_class']\n\n")
            f.write("# 2. ìµœì  Class Weight ë¡œë“œ\n")
            f.write("with open('optimal_class_weights.json', 'r', encoding='utf-8') as f:\n")
            f.write("    config = json.load(f)\n")
            f.write("optimal_weights = config['optimal_class_weights']\n\n")
            f.write("# 3. ëª¨ë¸ í›ˆë ¨\n")
            f.write("model = RandomForestClassifier(\n")
            f.write("    n_estimators=100,\n")
            f.write("    random_state=42,\n")
            f.write("    class_weight=optimal_weights,\n")
            f.write("    max_depth=8,\n")
            f.write("    min_samples_split=50,\n")
            f.write("    min_samples_leaf=20,\n")
            f.write("    max_features='sqrt'\n")
            f.write(")\n")
            f.write("model.fit(X_train, y_train)\n\n")
            f.write("# 4. ìƒˆ ë°ì´í„° ì˜ˆì¸¡ì‹œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©\n")
            f.write("with open('feature_scaler.pkl', 'rb') as f:\n")
            f.write("    scaler = pickle.load(f)\n")
            f.write("# new_data_scaled = scaler.transform(new_data)\n")
            f.write("```\n\n")
            f.write(f"## ğŸ“ˆ ìµœì  ì„¤ì • ì •ë³´\n\n")
            f.write(f"- **ìµœì  ì „ëµ**: {best_strategy}\n")
            f.write(f"- **F1-Score**: {results[best_strategy]['test_f1_macro']:.4f}\n")
            if best_strategy != 'baseline':
                baseline_f1 = results['baseline']['test_f1_macro']
                improvement = ((results[best_strategy]['test_f1_macro'] - baseline_f1) / baseline_f1) * 100
                f.write(f"- **ì„±ëŠ¥ ê°œì„ **: {improvement:+.1f}%\n")
            f.write(f"- **êµì°¨ê²€ì¦ ì•ˆì •ì„±**: Â±{results[best_strategy]['cv_std']*2:.4f}\n\n")
            f.write("## ğŸ¯ í´ë˜ìŠ¤ ì •ë³´\n\n")
            for cls_id, cls_name in self.class_names.items():
                count = y.value_counts().get(cls_id, 0)
                percentage = (count / len(y)) * 100
                weight = class_weights_dict.get(cls_id, 1.0) if class_weights_dict else 1.0
                f.write(f"- **í´ë˜ìŠ¤ {cls_id}** ({cls_name}): {count:,}ê±´ ({percentage:.1f}%), ê°€ì¤‘ì¹˜: {weight:.2f}\n")
        
        print(f"ğŸ“– ì‚¬ìš© ê°€ì´ë“œ ì €ì¥: {guide_file}")
        
        return {
            'dataset_file': dataset_file,
            'config_file': config_file,
            'train_file': train_file,
            'test_file': test_file,
            'scaler_file': scaler_file,
            'guide_file': guide_file
        }

if __name__ == "__main__":
    # ê³ ë˜ ê±°ë˜ íƒì§€ ë¶„ì„ ì‹¤í–‰
    analyzer = WhaleDetectionAnalyzer()
    analyzer.run_analysis() 