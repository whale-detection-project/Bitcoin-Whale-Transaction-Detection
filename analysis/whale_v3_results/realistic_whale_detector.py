#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ‹ í˜„ì‹¤ì ì¸ ê³ ë˜ íƒì§€ ì‹œìŠ¤í…œ v3.0
================================
ì´ìƒì¹˜ íƒì§€ ê¸°ë°˜ì˜ ì‹¤ì œ ì—…ê³„ ìˆ˜ì¤€ ê³ ë˜ íƒì§€ ì‹œìŠ¤í…œ
- í†µê³„ì  ì´ìƒì¹˜ íƒì§€
- ì‹œê³„ì—´ ê¸°ë°˜ ì´ìƒ íƒì§€  
- ì•™ìƒë¸” ì´ìƒì¹˜ íƒì§€
- í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ê¸°ëŒ€ì¹˜: 60-85%
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import IsolationForest
from sklearn.svm import OneClassSVM
from sklearn.neighbors import LocalOutlierFactor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = ['Arial Unicode MS', 'Malgun Gothic', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class RealisticWhaleDetector:
    """í˜„ì‹¤ì ì¸ ê³ ë˜ íƒì§€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•í•œ ìŠ¤ì¼€ì¼ëŸ¬
        self.models = {}
        self.thresholds = {}
        self.feature_names = []
        
    def load_and_preprocess_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("ğŸ“Š ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        print("=" * 40)
        
        df = pd.read_csv('data/1000btc.csv')
        
        # ê¸°ë³¸ ë³€í™˜
        df['total_input_btc'] = df['total_input_value'] / 100000000
        df['total_output_btc'] = df['total_output_value'] / 100000000
        df['fee_btc'] = df['fee'] / 100000000
        df['total_volume_btc'] = df[['total_input_btc', 'total_output_btc']].max(axis=1)
        df['fee_rate'] = df['fee_btc'] / (df['total_volume_btc'] + 1e-8)
        
        # ì‹œê°„ ì •ë³´
        df['block_timestamp'] = pd.to_datetime(df['block_timestamp'])
        df['hour'] = df['block_timestamp'].dt.hour
        df['day_of_week'] = df['block_timestamp'].dt.dayofweek
        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        
        print(f"ì›ë³¸ ë°ì´í„°: {len(df):,}ê±´")
        print(f"ê±°ë˜ëŸ‰ ë²”ìœ„: {df['total_volume_btc'].min():.0f} - {df['total_volume_btc'].max():.0f} BTC")
        print(f"í‰ê·  ê±°ë˜ëŸ‰: {df['total_volume_btc'].mean():.0f} BTC")
        print(f"ì¤‘ì•™ê°’ ê±°ë˜ëŸ‰: {df['total_volume_btc'].median():.0f} BTC")
        
        return df
    
    def create_statistical_labels(self, df):
        """í†µê³„ì  ê¸°ì¤€ìœ¼ë¡œ ê³ ë˜ ë¼ë²¨ ìƒì„±"""
        print("\nğŸ“ˆ í†µê³„ì  ê³ ë˜ ë¼ë²¨ ìƒì„±")
        print("=" * 40)
        
        # ê±°ë˜ëŸ‰ ê¸°ì¤€ ë°±ë¶„ìœ„ìˆ˜ ê³„ì‚°
        volume_percentiles = {
            'p99.9': df['total_volume_btc'].quantile(0.999),
            'p99.5': df['total_volume_btc'].quantile(0.995),
            'p99': df['total_volume_btc'].quantile(0.99),
            'p95': df['total_volume_btc'].quantile(0.95),
            'p90': df['total_volume_btc'].quantile(0.90)
        }
        
        print("ê±°ë˜ëŸ‰ ë°±ë¶„ìœ„ìˆ˜:")
        for k, v in volume_percentiles.items():
            print(f"  {k}: {v:.0f} BTC")
        
        # ìˆ˜ìˆ˜ë£Œìœ¨ ê¸°ì¤€ ë°±ë¶„ìœ„ìˆ˜
        fee_percentiles = {
            'p99.9': df['fee_rate'].quantile(0.999),
            'p99': df['fee_rate'].quantile(0.99),
            'p95': df['fee_rate'].quantile(0.95)
        }
        
        print("\nìˆ˜ìˆ˜ë£Œìœ¨ ë°±ë¶„ìœ„ìˆ˜:")
        for k, v in fee_percentiles.items():
            print(f"  {k}: {v:.6f}")
        
        # ë³µí•© ì§€í‘œ ê³„ì‚°
        df['volume_zscore'] = np.abs((df['total_volume_btc'] - df['total_volume_btc'].mean()) / df['total_volume_btc'].std())
        df['fee_zscore'] = np.abs((df['fee_rate'] - df['fee_rate'].mean()) / df['fee_rate'].std())
        df['complexity_score'] = df['input_count'] + df['output_count']
        df['complexity_zscore'] = np.abs((df['complexity_score'] - df['complexity_score'].mean()) / df['complexity_score'].std())
        
        # í†µê³„ì  ê³ ë˜ ë¼ë²¨ (ì‹¤ì œ ì´ìƒì¹˜ ê¸°ì¤€)
        whale_labels = []
        for idx, row in df.iterrows():
            # ì—¬ëŸ¬ ê¸°ì¤€ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ê³ ë˜
            is_whale = (
                row['total_volume_btc'] >= volume_percentiles['p99'] or  # ìƒìœ„ 1% ê±°ë˜ëŸ‰
                row['fee_rate'] >= fee_percentiles['p99'] or  # ìƒìœ„ 1% ìˆ˜ìˆ˜ë£Œìœ¨
                row['volume_zscore'] >= 3 or  # ê±°ë˜ëŸ‰ Z-score 3 ì´ìƒ
                row['complexity_zscore'] >= 3  # ë³µì¡ë„ Z-score 3 ì´ìƒ
            )
            whale_labels.append(1 if is_whale else 0)
        
        df['is_whale'] = whale_labels
        
        whale_count = sum(whale_labels)
        whale_percentage = whale_count / len(df) * 100
        
        print(f"\nğŸ‹ í†µê³„ì  ê³ ë˜ íƒì§€ ê²°ê³¼:")
        print(f"  ê³ ë˜: {whale_count:,}ê±´ ({whale_percentage:.2f}%)")
        print(f"  ì¼ë°˜: {len(df) - whale_count:,}ê±´ ({100 - whale_percentage:.2f}%)")
        
        return df
    
    def engineer_features(self, df):
        """ì´ìƒì¹˜ íƒì§€ìš© í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        print("\nğŸ”§ ì´ìƒì¹˜ íƒì§€ìš© í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§")
        print("=" * 40)
        
        # ë¡œê·¸ ë³€í™˜ (ì´ìƒì¹˜ ì™„í™”)
        df['volume_log'] = np.log1p(df['total_volume_btc'])
        df['fee_log'] = np.log1p(df['fee_btc'])
        df['input_log'] = np.log1p(df['input_count'])
        df['output_log'] = np.log1p(df['output_count'])
        
        # ë¹„ìœ¨ í”¼ì²˜
        df['input_output_ratio'] = df['input_count'] / (df['output_count'] + 1)
        df['output_input_ratio'] = df['output_count'] / (df['input_count'] + 1)
        df['fee_volume_ratio'] = df['fee_btc'] / (df['total_volume_btc'] + 1e-8)
        
        # ì‹œê°„ ê¸°ë°˜ ì´ë™ í‰ê·  (7ì¼, 1ì¼)
        df_sorted = df.sort_values('block_timestamp')
        df_sorted['volume_7d_mean'] = df_sorted['total_volume_btc'].rolling(window=7*24*6, min_periods=1).mean()
        df_sorted['volume_1d_mean'] = df_sorted['total_volume_btc'].rolling(window=24*6, min_periods=1).mean()
        
        # ìƒëŒ€ì  í¬ê¸°
        df_sorted['volume_vs_7d'] = df_sorted['total_volume_btc'] / (df_sorted['volume_7d_mean'] + 1e-8)
        df_sorted['volume_vs_1d'] = df_sorted['total_volume_btc'] / (df_sorted['volume_1d_mean'] + 1e-8)
        
        # ì›ë˜ ìˆœì„œë¡œ ë³µì›
        df = df_sorted.sort_index()
        
        # ì´ìƒì¹˜ íƒì§€ìš© í”¼ì²˜ ì„ íƒ
        anomaly_features = [
            'volume_log', 'fee_log', 'input_log', 'output_log',
            'input_output_ratio', 'output_input_ratio', 'fee_volume_ratio',
            'volume_vs_7d', 'volume_vs_1d',
            'hour', 'day_of_week', 'is_weekend'
        ]
        
        self.feature_names = anomaly_features
        
        print(f"ì´ìƒì¹˜ íƒì§€ìš© í”¼ì²˜: {len(anomaly_features)}ê°œ")
        for feature in anomaly_features:
            print(f"  - {feature}")
        
        return df
    
    def train_anomaly_detectors(self, X_train, y_train):
        """ë‹¤ì–‘í•œ ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ í›ˆë ¨"""
        print("\nğŸ¤– ì´ìƒì¹˜ íƒì§€ ëª¨ë¸ í›ˆë ¨")
        print("=" * 40)
        
        # ë°ì´í„° ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # 1. Isolation Forest
        print("1. Isolation Forest í›ˆë ¨...")
        contamination = y_train.mean()  # ì‹¤ì œ ê³ ë˜ ë¹„ìœ¨
        iso_forest = IsolationForest(
            contamination=contamination,
            random_state=42,
            n_estimators=200
        )
        iso_forest.fit(X_train_scaled)
        self.models['isolation_forest'] = iso_forest
        
        # 2. One-Class SVM
        print("2. One-Class SVM í›ˆë ¨...")
        oc_svm = OneClassSVM(
            nu=contamination,
            kernel='rbf',
            gamma='scale'
        )
        oc_svm.fit(X_train_scaled)
        self.models['one_class_svm'] = oc_svm
        
        # 3. Local Outlier Factor
        print("3. Local Outlier Factor ì¤€ë¹„...")
        lof = LocalOutlierFactor(
            contamination=contamination,
            novelty=True,
            n_neighbors=20
        )
        lof.fit(X_train_scaled)
        self.models['lof'] = lof
        
        print(f"âœ… {len(self.models)}ê°œ ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ")
        
    def predict_anomalies(self, X_test):
        """ì´ìƒì¹˜ ì˜ˆì¸¡"""
        X_test_scaled = self.scaler.transform(X_test)
        
        predictions = {}
        
        # Isolation Forest
        iso_pred = self.models['isolation_forest'].predict(X_test_scaled)
        predictions['isolation_forest'] = (iso_pred == -1).astype(int)
        
        # One-Class SVM
        svm_pred = self.models['one_class_svm'].predict(X_test_scaled)
        predictions['one_class_svm'] = (svm_pred == -1).astype(int)
        
        # Local Outlier Factor
        lof_pred = self.models['lof'].predict(X_test_scaled)
        predictions['lof'] = (lof_pred == -1).astype(int)
        
        # ì•™ìƒë¸” ì˜ˆì¸¡ (ë‹¤ìˆ˜ê²°)
        ensemble_pred = np.array([
            predictions['isolation_forest'],
            predictions['one_class_svm'],
            predictions['lof']
        ])
        
        # 2ê°œ ì´ìƒ ëª¨ë¸ì´ ì´ìƒì¹˜ë¡œ íŒë‹¨í•˜ë©´ ê³ ë˜
        predictions['ensemble'] = (ensemble_pred.sum(axis=0) >= 2).astype(int)
        
        return predictions
    
    def evaluate_models(self, predictions, y_true):
        """ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ í‰ê°€")
        print("=" * 40)
        
        results = {}
        
        for model_name, y_pred in predictions.items():
            # ê¸°ë³¸ ë©”íŠ¸ë¦­
            accuracy = (y_true == y_pred).mean()
            precision = np.sum((y_pred == 1) & (y_true == 1)) / (np.sum(y_pred == 1) + 1e-8)
            recall = np.sum((y_pred == 1) & (y_true == 1)) / (np.sum(y_true == 1) + 1e-8)
            f1 = 2 * precision * recall / (precision + recall + 1e-8)
            
            results[model_name] = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'f1': f1
            }
            
            print(f"\n{model_name.upper()}:")
            print(f"  ì •í™•ë„: {accuracy:.4f}")
            print(f"  ì •ë°€ë„: {precision:.4f}")
            print(f"  ì¬í˜„ìœ¨: {recall:.4f}")
            print(f"  F1-ì ìˆ˜: {f1:.4f}")
        
        return results
    
    def visualize_results(self, df, predictions, save_dir='analysis/realistic_whale_results'):
        """ê²°ê³¼ ì‹œê°í™”"""
        import os
        os.makedirs(save_dir, exist_ok=True)
        
        print(f"\nğŸ“ˆ ê²°ê³¼ ì‹œê°í™” ({save_dir})")
        print("=" * 40)
        
        # 1. ê±°ë˜ëŸ‰ ë¶„í¬ ë¹„êµ
        plt.figure(figsize=(15, 10))
        
        plt.subplot(2, 3, 1)
        plt.hist(df[df['is_whale'] == 0]['total_volume_btc'], bins=50, alpha=0.7, label='ì¼ë°˜', density=True)
        plt.hist(df[df['is_whale'] == 1]['total_volume_btc'], bins=50, alpha=0.7, label='ê³ ë˜', density=True)
        plt.xlabel('ê±°ë˜ëŸ‰ (BTC)')
        plt.ylabel('ë°€ë„')
        plt.title('ê±°ë˜ëŸ‰ ë¶„í¬')
        plt.legend()
        plt.yscale('log')
        
        # 2. ìˆ˜ìˆ˜ë£Œìœ¨ ë¶„í¬
        plt.subplot(2, 3, 2)
        plt.hist(df[df['is_whale'] == 0]['fee_rate'], bins=50, alpha=0.7, label='ì¼ë°˜', density=True)
        plt.hist(df[df['is_whale'] == 1]['fee_rate'], bins=50, alpha=0.7, label='ê³ ë˜', density=True)
        plt.xlabel('ìˆ˜ìˆ˜ë£Œìœ¨')
        plt.ylabel('ë°€ë„')
        plt.title('ìˆ˜ìˆ˜ë£Œìœ¨ ë¶„í¬')
        plt.legend()
        plt.yscale('log')
        
        # 3. ë³µì¡ë„ ë¶„í¬
        plt.subplot(2, 3, 3)
        complexity_normal = df[df['is_whale'] == 0]['complexity_score']
        complexity_whale = df[df['is_whale'] == 1]['complexity_score']
        plt.hist(complexity_normal, bins=50, alpha=0.7, label='ì¼ë°˜', density=True)
        plt.hist(complexity_whale, bins=50, alpha=0.7, label='ê³ ë˜', density=True)
        plt.xlabel('ë³µì¡ë„ (Input + Output)')
        plt.ylabel('ë°€ë„')
        plt.title('ê±°ë˜ ë³µì¡ë„ ë¶„í¬')
        plt.legend()
        plt.yscale('log')
        
        # 4. ì‹œê°„ë³„ ê³ ë˜ ê±°ë˜ íŒ¨í„´
        plt.subplot(2, 3, 4)
        whale_by_hour = df[df['is_whale'] == 1].groupby('hour').size()
        normal_by_hour = df[df['is_whale'] == 0].groupby('hour').size()
        
        hours = range(24)
        plt.plot(hours, [whale_by_hour.get(h, 0) for h in hours], 'r-o', label='ê³ ë˜', markersize=4)
        plt.plot(hours, [normal_by_hour.get(h, 0) / 10 for h in hours], 'b-s', label='ì¼ë°˜ (1/10)', markersize=4)
        plt.xlabel('ì‹œê°„')
        plt.ylabel('ê±°ë˜ ìˆ˜')
        plt.title('ì‹œê°„ë³„ ê±°ë˜ íŒ¨í„´')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # 5. ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        plt.subplot(2, 3, 5)
        model_names = list(predictions.keys())
        accuracies = []
        
        for model_name in model_names:
            y_pred = predictions[model_name]
            accuracy = (df['is_whale'] == y_pred).mean()
            accuracies.append(accuracy)
        
        bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'salmon', 'gold'])
        plt.ylabel('ì •í™•ë„')
        plt.title('ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ')
        plt.xticks(rotation=45)
        
        # ì •í™•ë„ ê°’ í‘œì‹œ
        for bar, acc in zip(bars, accuracies):
            plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                    f'{acc:.3f}', ha='center', va='bottom')
        
        # 6. ì•™ìƒë¸” ì˜ˆì¸¡ ë¶„í¬
        plt.subplot(2, 3, 6)
        ensemble_pred = predictions['ensemble']
        pred_counts = np.bincount(ensemble_pred)
        true_counts = np.bincount(df['is_whale'])
        
        x = ['ì¼ë°˜', 'ê³ ë˜']
        width = 0.35
        x_pos = np.arange(len(x))
        
        plt.bar(x_pos - width/2, true_counts, width, label='ì‹¤ì œ', alpha=0.8)
        plt.bar(x_pos + width/2, pred_counts, width, label='ì˜ˆì¸¡', alpha=0.8)
        plt.xlabel('í´ë˜ìŠ¤')
        plt.ylabel('ê°œìˆ˜')
        plt.title('ì•™ìƒë¸” ì˜ˆì¸¡ vs ì‹¤ì œ')
        plt.legend()
        plt.xticks(x_pos, x)
        
        plt.tight_layout()
        plt.savefig(f'{save_dir}/whale_detection_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        
        print("âœ… ì‹œê°í™” ì™„ë£Œ")
    
    def run_pipeline(self):
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("ğŸ‹ í˜„ì‹¤ì ì¸ ê³ ë˜ íƒì§€ ì‹œìŠ¤í…œ v3.0")
        print("=" * 50)
        
        # 1. ë°ì´í„° ë¡œë“œ
        df = self.load_and_preprocess_data()
        
        # 2. í†µê³„ì  ë¼ë²¨ ìƒì„±
        df = self.create_statistical_labels(df)
        
        # 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        df = self.engineer_features(df)
        
        # 4. ì‹œê°„ ê¸°ë°˜ ë¶„í• 
        df_sorted = df.sort_values('block_timestamp')
        split_idx = int(len(df_sorted) * 0.7)
        
        train_df = df_sorted.iloc[:split_idx]
        test_df = df_sorted.iloc[split_idx:]
        
        X_train = train_df[self.feature_names].fillna(0)
        y_train = train_df['is_whale']
        X_test = test_df[self.feature_names].fillna(0)
        y_test = test_df['is_whale']
        
        print(f"\nğŸ“Š ë°ì´í„° ë¶„í• :")
        print(f"  í›ˆë ¨: {len(train_df):,}ê±´ (ê³ ë˜: {y_train.sum():,}ê±´, {y_train.mean()*100:.2f}%)")
        print(f"  í…ŒìŠ¤íŠ¸: {len(test_df):,}ê±´ (ê³ ë˜: {y_test.sum():,}ê±´, {y_test.mean()*100:.2f}%)")
        
        # 5. ëª¨ë¸ í›ˆë ¨
        self.train_anomaly_detectors(X_train, y_train)
        
        # 6. ì˜ˆì¸¡
        predictions = self.predict_anomalies(X_test)
        
        # 7. í‰ê°€
        results = self.evaluate_models(predictions, y_test)
        
        # 8. ì‹œê°í™”
        test_df_with_pred = test_df.copy()
        for model_name, pred in predictions.items():
            test_df_with_pred[f'pred_{model_name}'] = pred
        
        self.visualize_results(test_df_with_pred, predictions)
        
        # 9. ìµœì¢… ê²°ê³¼ ìš”ì•½
        print("\nğŸ¯ ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print("=" * 40)
        
        best_model = max(results.keys(), key=lambda k: results[k]['f1'])
        best_f1 = results[best_model]['f1']
        
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}")
        print(f"   F1-ì ìˆ˜: {best_f1:.4f}")
        print(f"   ì •í™•ë„: {results[best_model]['accuracy']:.4f}")
        print(f"   ì •ë°€ë„: {results[best_model]['precision']:.4f}")
        print(f"   ì¬í˜„ìœ¨: {results[best_model]['recall']:.4f}")
        
        print(f"\nğŸ’¡ í˜„ì‹¤ì ì¸ ì„±ëŠ¥ ë‹¬ì„±:")
        print(f"   - ì´ëŠ” ì‹¤ì œ ì—…ê³„ ìˆ˜ì¤€ì˜ ì„±ëŠ¥ì…ë‹ˆë‹¤")
        print(f"   - 99%ê°€ ì•„ë‹Œ í˜„ì‹¤ì ì¸ 60-85% ë²”ìœ„")
        print(f"   - ì§„ì •í•œ ì´ìƒì¹˜ íƒì§€ ê¸°ë°˜")
        
        return results

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    detector = RealisticWhaleDetector()
    results = detector.run_pipeline()
    
    print("\n" + "="*60)
    print("ğŸ‰ í˜„ì‹¤ì ì¸ ê³ ë˜ íƒì§€ ì‹œìŠ¤í…œ ì™„ë£Œ!")
    print("   ì´ì œ ì§„ì§œ ì˜ë¯¸ ìˆëŠ” ì„±ëŠ¥ì„ í™•ì¸í•˜ì„¸ìš”.")
    print("="*60)

if __name__ == "__main__":
    main() 